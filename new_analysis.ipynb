{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import time\n",
    "\n",
    "# MongoDB连接\n",
    "mongo_uri = \"mongodb+srv://fernando:Zz12345678@stockanalysis.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000\" # 请替换为你的MongoDB连接字符串\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client['stock_data']\n",
    "collection = db['all_stocks_ticks']\n",
    "\n",
    "# 创建索引以优化查询（如果尚未创建）\n",
    "collection.create_index([('time', 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_timestamps = collection.distinct(\"time\")\n",
    "unique_timestamps.sort()  # 确保按时间顺序处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 1/30238 [00:00<5:24:14,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402900000 with 177 records.\n",
      "Processing records for timestamp: 1713402901000 with 289 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 3/30238 [00:01<2:55:25,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402902000 with 274 records.\n",
      "Processing records for timestamp: 1713402903000 with 332 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 5/30238 [00:01<1:59:49,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402904000 with 316 records.\n",
      "Processing records for timestamp: 1713402905000 with 280 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 7/30238 [00:01<1:36:09,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402906000 with 337 records.\n",
      "Processing records for timestamp: 1713402907000 with 317 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 9/30238 [00:02<1:29:56,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402908000 with 300 records.\n",
      "Processing records for timestamp: 1713402909000 with 319 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 11/30238 [00:02<1:33:31,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402910000 with 330 records.\n",
      "Processing records for timestamp: 1713402911000 with 280 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 12/30238 [00:02<1:33:31,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402912000 with 333 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 14/30238 [00:03<1:51:49,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402913000 with 306 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 15/30238 [00:03<1:48:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402914000 with 288 records.\n",
      "Processing records for timestamp: 1713402915000 with 273 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 17/30238 [00:03<1:45:44,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402916000 with 248 records.\n",
      "Processing records for timestamp: 1713402917000 with 167 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 20/30238 [00:04<1:14:29,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402918000 with 98 records.\n",
      "Processing records for timestamp: 1713402919000 with 98 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 21/30238 [00:04<1:09:47,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402920000 with 99 records.\n",
      "Processing records for timestamp: 1713402921000 with 102 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 24/30238 [00:04<1:11:37,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402922000 with 94 records.\n",
      "Processing records for timestamp: 1713402923000 with 102 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 25/30238 [00:04<1:05:23,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402924000 with 100 records.\n",
      "Processing records for timestamp: 1713402925000 with 103 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 28/30238 [00:05<1:09:01,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402926000 with 99 records.\n",
      "Processing records for timestamp: 1713402927000 with 110 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 29/30238 [00:05<1:12:14,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402928000 with 103 records.\n",
      "Processing records for timestamp: 1713402929000 with 94 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 31/30238 [00:05<1:14:05,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402930000 with 111 records.\n",
      "Processing records for timestamp: 1713402931000 with 115 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 33/30238 [00:06<1:11:39,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402932000 with 98 records.\n",
      "Processing records for timestamp: 1713402933000 with 110 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 35/30238 [00:06<1:21:18,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402934000 with 128 records.\n",
      "Processing records for timestamp: 1713402935000 with 122 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 37/30238 [00:06<1:19:11,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402936000 with 122 records.\n",
      "Processing records for timestamp: 1713402937000 with 105 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 39/30238 [00:07<1:19:44,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402938000 with 116 records.\n",
      "Processing records for timestamp: 1713402939000 with 119 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 41/30238 [00:07<1:23:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402940000 with 117 records.\n",
      "Processing records for timestamp: 1713402941000 with 112 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 43/30238 [00:07<1:26:14,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402942000 with 130 records.\n",
      "Processing records for timestamp: 1713402943000 with 122 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 45/30238 [00:08<1:20:56,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402944000 with 124 records.\n",
      "Processing records for timestamp: 1713402945000 with 138 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 47/30238 [00:08<1:20:54,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402946000 with 123 records.\n",
      "Processing records for timestamp: 1713402947000 with 118 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 49/30238 [00:08<1:18:39,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402948000 with 141 records.\n",
      "Processing records for timestamp: 1713402949000 with 120 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 51/30238 [00:09<1:24:25,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402950000 with 105 records.\n",
      "Processing records for timestamp: 1713402951000 with 141 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 53/30238 [00:09<1:22:41,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402952000 with 110 records.\n",
      "Processing records for timestamp: 1713402953000 with 125 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 55/30238 [00:09<1:28:32,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402954000 with 131 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 56/30238 [00:09<1:27:34,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402955000 with 120 records.\n",
      "Processing records for timestamp: 1713402956000 with 113 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 58/30238 [00:10<1:29:33,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402957000 with 136 records.\n",
      "Processing records for timestamp: 1713402958000 with 120 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 60/30238 [00:10<1:26:57,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402959000 with 115 records.\n",
      "Processing records for timestamp: 1713402960000 with 113 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 62/30238 [00:10<1:28:03,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402961000 with 118 records.\n",
      "Processing records for timestamp: 1713402962000 with 113 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 64/30238 [00:11<1:23:54,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402963000 with 138 records.\n",
      "Processing records for timestamp: 1713402964000 with 110 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 66/30238 [00:11<1:22:57,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402965000 with 134 records.\n",
      "Processing records for timestamp: 1713402966000 with 142 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 68/30238 [00:11<1:20:34,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402967000 with 138 records.\n",
      "Processing records for timestamp: 1713402968000 with 125 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 70/30238 [00:12<1:22:53,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402969000 with 119 records.\n",
      "Processing records for timestamp: 1713402970000 with 110 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 72/30238 [00:12<1:20:59,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402971000 with 124 records.\n",
      "Processing records for timestamp: 1713402972000 with 137 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 74/30238 [00:12<1:23:59,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402973000 with 132 records.\n",
      "Processing records for timestamp: 1713402974000 with 124 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 76/30238 [00:13<1:25:05,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402975000 with 141 records.\n",
      "Processing records for timestamp: 1713402976000 with 136 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:   0%|          | 78/30238 [00:13<1:27:43,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing records for timestamp: 1713402977000 with 136 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestamp \u001b[38;5;129;01min\u001b[39;00m tqdm(unique_timestamps, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# 对于每个时间戳，查询所有相关的记录\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     query \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: timestamp}\n\u001b[1;32m----> 6\u001b[0m     record_count \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mcount_documents(query)  \u001b[38;5;66;03m# 获取记录数量\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     records \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mfind(query)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing records for timestamp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\collection.py:1913\u001b[0m, in \u001b[0;36mCollection.count_documents\u001b[1;34m(self, filter, session, comment, **kwargs)\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 1913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retryable_non_cursor_read(_cmd, session)\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\collection.py:1923\u001b[0m, in \u001b[0;36mCollection._retryable_non_cursor_read\u001b[1;34m(self, func, session)\u001b[0m\n\u001b[0;32m   1921\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__database\u001b[38;5;241m.\u001b[39mclient\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m client\u001b[38;5;241m.\u001b[39m_tmp_session(session) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[1;32m-> 1923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39m_retryable_read(func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_preference_for(s), s)\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\mongo_client.py:1492\u001b[0m, in \u001b[0;36mMongoClient._retryable_read\u001b[1;34m(self, func, read_pref, session, address, retryable)\u001b[0m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;66;03m# Ensure that the client supports retrying on reads and there is no session in\u001b[39;00m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this call.\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m retryable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\n\u001b[0;32m   1490\u001b[0m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mretry_reads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (session \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39min_transaction)\n\u001b[0;32m   1491\u001b[0m )\n\u001b[1;32m-> 1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_internal(\n\u001b[0;32m   1493\u001b[0m     func,\n\u001b[0;32m   1494\u001b[0m     session,\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1496\u001b[0m     is_read\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1497\u001b[0m     address\u001b[38;5;241m=\u001b[39maddress,\n\u001b[0;32m   1498\u001b[0m     read_pref\u001b[38;5;241m=\u001b[39mread_pref,\n\u001b[0;32m   1499\u001b[0m     retryable\u001b[38;5;241m=\u001b[39mretryable,\n\u001b[0;32m   1500\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\_csot.py:107\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\mongo_client.py:1462\u001b[0m, in \u001b[0;36mMongoClient._retry_internal\u001b[1;34m(self, func, session, bulk, is_read, address, read_pref, retryable)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;129m@_csot\u001b[39m\u001b[38;5;241m.\u001b[39mapply\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_retry_internal\u001b[39m(\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1437\u001b[0m     retryable: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1438\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m \n\u001b[0;32m   1441\u001b[0m \u001b[38;5;124;03m    :Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;124;03m      Output of the calling func()\u001b[39;00m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClientConnectionRetryable(\n\u001b[0;32m   1454\u001b[0m         mongo_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1455\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   1456\u001b[0m         bulk\u001b[38;5;241m=\u001b[39mbulk,\n\u001b[0;32m   1457\u001b[0m         is_read\u001b[38;5;241m=\u001b[39mis_read,\n\u001b[0;32m   1458\u001b[0m         session\u001b[38;5;241m=\u001b[39msession,\n\u001b[0;32m   1459\u001b[0m         read_pref\u001b[38;5;241m=\u001b[39mread_pref,\n\u001b[0;32m   1460\u001b[0m         address\u001b[38;5;241m=\u001b[39maddress,\n\u001b[0;32m   1461\u001b[0m         retryable\u001b[38;5;241m=\u001b[39mretryable,\n\u001b[1;32m-> 1462\u001b[0m     )\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\mongo_client.py:2315\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error(check_csot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write()\n\u001b[0;32m   2316\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[0;32m   2317\u001b[0m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[0;32m   2318\u001b[0m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[0;32m   2319\u001b[0m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[0;32m   2320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\mongo_client.py:2445\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable._read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrying \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retryable:\n\u001b[0;32m   2444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n\u001b[1;32m-> 2445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server, conn, read_pref)\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\collection.py:1908\u001b[0m, in \u001b[0;36mCollection.count_documents.<locals>._cmd\u001b[1;34m(session, _server, conn, read_preference)\u001b[0m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cmd\u001b[39m(\n\u001b[0;32m   1903\u001b[0m     session: Optional[ClientSession],\n\u001b[0;32m   1904\u001b[0m     _server: Server,\n\u001b[0;32m   1905\u001b[0m     conn: Connection,\n\u001b[0;32m   1906\u001b[0m     read_preference: Optional[_ServerMode],\n\u001b[0;32m   1907\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m-> 1908\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_one_result(conn, read_preference, cmd, collation, session)\n\u001b[0;32m   1909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[0;32m   1910\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\collection.py:1764\u001b[0m, in \u001b[0;36mCollection._aggregate_one_result\u001b[1;34m(self, conn, read_preference, cmd, collation, session)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aggregate_one_result\u001b[39m(\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1757\u001b[0m     conn: Connection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1761\u001b[0m     session: Optional[ClientSession],\n\u001b[0;32m   1762\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal helper to run an aggregate that returns a single result.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1764\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command(\n\u001b[0;32m   1765\u001b[0m         conn,\n\u001b[0;32m   1766\u001b[0m         cmd,\n\u001b[0;32m   1767\u001b[0m         read_preference,\n\u001b[0;32m   1768\u001b[0m         allowable_errors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m26\u001b[39m],  \u001b[38;5;66;03m# Ignore NamespaceNotFound.\u001b[39;00m\n\u001b[0;32m   1769\u001b[0m         codec_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__write_response_codec_options,\n\u001b[0;32m   1770\u001b[0m         read_concern\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_concern,\n\u001b[0;32m   1771\u001b[0m         collation\u001b[38;5;241m=\u001b[39mcollation,\n\u001b[0;32m   1772\u001b[0m         session\u001b[38;5;241m=\u001b[39msession,\n\u001b[0;32m   1773\u001b[0m     )\n\u001b[0;32m   1774\u001b[0m     \u001b[38;5;66;03m# cursor will not be present for NamespaceNotFound errors.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcursor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\collection.py:308\u001b[0m, in \u001b[0;36mCollection._command\u001b[1;34m(self, conn, command, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, collation, session, retryable_write, user_fields)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Internal command helper.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m:Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m  The result document.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__database\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_tmp_session(session) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mcommand(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__database\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    310\u001b[0m         command,\n\u001b[0;32m    311\u001b[0m         read_preference \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_preference_for(session),\n\u001b[0;32m    312\u001b[0m         codec_options \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodec_options,\n\u001b[0;32m    313\u001b[0m         check,\n\u001b[0;32m    314\u001b[0m         allowable_errors,\n\u001b[0;32m    315\u001b[0m         read_concern\u001b[38;5;241m=\u001b[39mread_concern,\n\u001b[0;32m    316\u001b[0m         write_concern\u001b[38;5;241m=\u001b[39mwrite_concern,\n\u001b[0;32m    317\u001b[0m         parse_write_concern_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    318\u001b[0m         collation\u001b[38;5;241m=\u001b[39mcollation,\n\u001b[0;32m    319\u001b[0m         session\u001b[38;5;241m=\u001b[39ms,\n\u001b[0;32m    320\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__database\u001b[38;5;241m.\u001b[39mclient,\n\u001b[0;32m    321\u001b[0m         retryable_write\u001b[38;5;241m=\u001b[39mretryable_write,\n\u001b[0;32m    322\u001b[0m         user_fields\u001b[38;5;241m=\u001b[39muser_fields,\n\u001b[0;32m    323\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\helpers.py:322\u001b[0m, in \u001b[0;36m_handle_reauth.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Connection\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_reauth:\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\pool.py:996\u001b[0m, in \u001b[0;36mConnection.command\u001b[1;34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m--> 996\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_connection_failure(error)\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\pool.py:968\u001b[0m, in \u001b[0;36mConnection.command\u001b[1;34m(self, dbname, spec, read_preference, codec_options, check, allowable_errors, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_not_writable(unacknowledged)\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m command(\n\u001b[0;32m    969\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    970\u001b[0m         dbname,\n\u001b[0;32m    971\u001b[0m         spec,\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mongos,\n\u001b[0;32m    973\u001b[0m         read_preference,\n\u001b[0;32m    974\u001b[0m         codec_options,\n\u001b[0;32m    975\u001b[0m         session,\n\u001b[0;32m    976\u001b[0m         client,\n\u001b[0;32m    977\u001b[0m         check,\n\u001b[0;32m    978\u001b[0m         allowable_errors,\n\u001b[0;32m    979\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress,\n\u001b[0;32m    980\u001b[0m         listeners,\n\u001b[0;32m    981\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bson_size,\n\u001b[0;32m    982\u001b[0m         read_concern,\n\u001b[0;32m    983\u001b[0m         parse_write_concern_error\u001b[38;5;241m=\u001b[39mparse_write_concern_error,\n\u001b[0;32m    984\u001b[0m         collation\u001b[38;5;241m=\u001b[39mcollation,\n\u001b[0;32m    985\u001b[0m         compression_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression_context,\n\u001b[0;32m    986\u001b[0m         use_op_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop_msg_enabled,\n\u001b[0;32m    987\u001b[0m         unacknowledged\u001b[38;5;241m=\u001b[39munacknowledged,\n\u001b[0;32m    988\u001b[0m         user_fields\u001b[38;5;241m=\u001b[39muser_fields,\n\u001b[0;32m    989\u001b[0m         exhaust_allowed\u001b[38;5;241m=\u001b[39mexhaust_allowed,\n\u001b[0;32m    990\u001b[0m         write_concern\u001b[38;5;241m=\u001b[39mwrite_concern,\n\u001b[0;32m    991\u001b[0m     )\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (OperationFailure, NotPrimaryError):\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\network.py:182\u001b[0m, in \u001b[0;36mcommand\u001b[1;34m(conn, dbname, spec, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed, write_concern)\u001b[0m\n\u001b[0;32m    180\u001b[0m     response_doc: _DocumentOut \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     reply \u001b[38;5;241m=\u001b[39m receive_message(conn, request_id)\n\u001b[0;32m    183\u001b[0m     conn\u001b[38;5;241m.\u001b[39mmore_to_come \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39mmore_to_come\n\u001b[0;32m    184\u001b[0m     unpacked_docs \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39munpack_response(\n\u001b[0;32m    185\u001b[0m         codec_options\u001b[38;5;241m=\u001b[39mcodec_options, user_fields\u001b[38;5;241m=\u001b[39muser_fields\n\u001b[0;32m    186\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\network.py:257\u001b[0m, in \u001b[0;36mreceive_message\u001b[1;34m(conn, request_id, max_message_size)\u001b[0m\n\u001b[0;32m    255\u001b[0m         deadline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Ignore the response's request id.\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m length, _, response_to, op_code \u001b[38;5;241m=\u001b[39m _UNPACK_HEADER(_receive_data_on_socket(conn, \u001b[38;5;241m16\u001b[39m, deadline))\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# No request_id for exhaust cursor \"getMore\".\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\site-packages\\pymongo\\network.py:340\u001b[0m, in \u001b[0;36m_receive_data_on_socket\u001b[1;34m(conn, length, deadline)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _csot\u001b[38;5;241m.\u001b[39mget_timeout() \u001b[38;5;129;01mand\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m         conn\u001b[38;5;241m.\u001b[39mset_conn_timeout(\u001b[38;5;28mmax\u001b[39m(deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic(), \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m--> 340\u001b[0m     chunk_length \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mrecv_into(mv[bytes_read:])\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BLOCKING_IO_ERRORS:\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimed out\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Reed\\anaconda3\\envs\\qmt_env\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # 引入tqdm\n",
    "# 使用tqdm在遍历每个唯一的时间戳时显示进度条\n",
    "for timestamp in tqdm(unique_timestamps, desc=\"Processing timestamps\"):\n",
    "    # 对于每个时间戳，查询所有相关的记录\n",
    "    query = {\"time\": timestamp}\n",
    "    record_count = collection.count_documents(query)  # 获取记录数量\n",
    "    records = collection.find(query)\n",
    "\n",
    "    #print(f\"Processing records for timestamp: {timestamp} with {record_count} records.\")\n",
    "    for record in records:\n",
    "        pass # 现在希望kafka发送数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bson import ObjectId\n",
    "\n",
    "def json_serializer(data):\n",
    "    \"\"\"自定义JSON序列化函数，处理MongoDB特有的数据类型.\"\"\"\n",
    "    return json.dumps(data, default=json_util_default).encode('utf-8')\n",
    "\n",
    "def json_util_default(obj):\n",
    "    \"\"\"将无法直接序列化的对象转换为可序列化的格式.\"\"\"\n",
    "    if isinstance(obj, ObjectId):\n",
    "        return str(obj)  # 将ObjectId转换为字符串\n",
    "    raise TypeError(\"不可序列化的对象类型：%s\" % type(obj).__name__)\n",
    "\n",
    "# 设置 Kafka 生产者，使用自定义的序列化函数\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=json_serializer  # 使用自定义的JSON序列化函数\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing timestamps:  62%|██████▏   | 18714/30238 [5:14:43<4:43:53,  1.48s/it] "
     ]
    }
   ],
   "source": [
    "# 使用tqdm在遍历每个唯一的时间戳时显示进度条\n",
    "for timestamp in tqdm(unique_timestamps, desc=\"Processing timestamps\"):\n",
    "    query = {\"time\": timestamp}\n",
    "    records = collection.find(query)\n",
    "\n",
    "    for record in records:\n",
    "        # 发送数据到Kafka的特定主题\n",
    "        producer.send('stock_prices', record)  # 替换'your_topic_name'为你的Kafka主题\n",
    "        producer.flush()  # 确保所有消息都已被发送"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qmt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
